{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:50:07.220935Z",
     "start_time": "2025-02-04T16:50:00.011630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from idlelib.iomenu import encoding\n",
    "\n",
    "from sympy.printing.tensorflow import tensorflow\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'distilbert-base-uncased'\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)  # 6 emotions\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example emotion labels: [\"happy\", \"sad\", \"angry\", \"anxious\", \"neutral\", \"excited\"]\n",
    "emotion_labels = [\"happy\", \"sad\", \"angry\", \"anxious\", \"neutral\", \"excited\"]\n"
   ],
   "id": "2efa62bf978df388",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Suman Kar\\PycharmProjects\\slm-model\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Suman Kar\\PycharmProjects\\slm-model\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:50:13.512318Z",
     "start_time": "2025-02-04T16:50:13.508103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    encodings = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    return encodings"
   ],
   "id": "b4acc62cd9382258",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:50:15.358844Z",
     "start_time": "2025-02-04T16:50:15.352979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_emotion(text):\n",
    "    inputs=preprocess_text(text)\n",
    "    outputs=model(inputs)\n",
    "    logits=outputs.logits\n",
    "    predicted_class=tf.argmax(logits, axis=1)\n",
    "    return emotion_labels[predicted_class.numpy()[0]]"
   ],
   "id": "7d9802cf4af6a92",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:50:21.490544Z",
     "start_time": "2025-02-04T16:50:21.381472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_response(emotion):\n",
    "    responses = {\n",
    "        \"happy\": \"I'm glad you're feeling good! ðŸ˜Š What's making you so happy?\",\n",
    "        \"sad\": \"I'm sorry to hear that. Want to talk about what's on your mind? ðŸ’™\",\n",
    "        \"angry\": \"I understand that you're upset. Take a deep breath. ðŸ˜Œ\",\n",
    "        \"anxious\": \"It's okay to feel anxious. Would you like to share more? ðŸ’­\",\n",
    "        \"neutral\": \"I see you're feeling neutral. What's going on? ðŸ¤”\",\n",
    "        \"excited\": \"Thatâ€™s awesome! Tell me more about whatâ€™s got you excited! ðŸŽ‰\"\n",
    "    }\n",
    "\n",
    "    return responses.get(emotion, \"Iâ€™m here to listen! ðŸ˜Š\")\n",
    "\n",
    "# Example interaction\n",
    "user_input = \"I'm feeling a bit overwhelmed today.\"\n",
    "predicted_emotion = predict_emotion(user_input)\n",
    "response = generate_response(predicted_emotion)\n",
    "print(f\"Emotion: {predicted_emotion}\")\n",
    "print(f\"Response: {response}\")"
   ],
   "id": "d2e3a38b353eb998",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: happy\n",
      "Response: I'm glad you're feeling good! ðŸ˜Š What's making you so happy?\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:56:45.098053Z",
     "start_time": "2025-02-04T16:56:45.055184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ],
   "id": "20d854b76cc07a80",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x00000241F3093230>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[1;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlosses\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSparseCategoricalCrossentropy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfrom_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\slm-model\\.venv\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1563\u001B[0m, in \u001B[0;36mTFPreTrainedModel.compile\u001B[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001B[0m\n\u001B[0;32m   1561\u001B[0m \u001B[38;5;66;03m# This argument got renamed, we need to support both versions\u001B[39;00m\n\u001B[0;32m   1562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps_per_execution\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m parent_args:\n\u001B[1;32m-> 1563\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1565\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1566\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1567\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1568\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweighted_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweighted_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1569\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_eagerly\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_eagerly\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1570\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_execution\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1571\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1572\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1573\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1574\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m   1575\u001B[0m         optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[0;32m   1576\u001B[0m         loss\u001B[38;5;241m=\u001B[39mloss,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1582\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1583\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\slm-model\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\PycharmProjects\\slm-model\\.venv\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:335\u001B[0m, in \u001B[0;36mget\u001B[1;34m(identifier, **kwargs)\u001B[0m\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get(\n\u001B[0;32m    331\u001B[0m         config,\n\u001B[0;32m    332\u001B[0m         use_legacy_optimizer\u001B[38;5;241m=\u001B[39muse_legacy_optimizer,\n\u001B[0;32m    333\u001B[0m     )\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 335\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    336\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not interpret optimizer identifier: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midentifier\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    337\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x00000241F3093230>"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=16)\n"
   ],
   "id": "7ad752294ff9cced"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
